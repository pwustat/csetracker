---
title: 'ASA Webinar: Interpreting Deep Neural Networks towards Trustworthiness '
author: ''
date: '2022-03-15T09:00:00-07:00'
slug: asa-webinar-interpreting-deep-neural-networks-towards-trustworthiness
categories: []
tags: []
type: webinar
url_register: https://www.amstat.org/education/web-based-lectures#IDNT
url_freeregister: ~
url_slides: ~
url_video: no
url_agenda: ~
url_website: ~
url_audio: ~
url_code: ~
url_pdf: ~
date_end: '2022-03-15T10:00:00-07:00'
all_day: no
publishDate: '2022-03-09T09:32:54-08:00'
authors: []
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
slides: ''
projects: []
location: ~
address:
  street: ~
  city: ~
  region: ~
  postcode: ~
  country: ~
summary: ~
abstract: ~
speaker: Bin Yu (UC Berkeley)
---
<span style="color: salmon;">*ASA members: $30; non-members: $45*</span>
<!--more-->
Recent deep learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. In this talk, I will begin with a discussion on defining interpretable machine learning in general and describe our agglomerative contextual decomposition (ACD) method to interpret neural networks. ACD attributes importance to features and feature interactions for individual predictions and has brought insights to NLP/computer vision problems and can be used to directly improve generalization in interesting ways.  

I will then focus on scientific interpretable machine learning. Building on ACD's extension to the scientifically meaningful frequency domain, an adaptive wavelet distillation (AWD) interpretation method is developed. AWD is shown to be both outperforming deep neural networks and interpretable in two prediction problems from cosmology and cell biology. Finally, I will address the need to quality-control the entire data science life cycle to build any model for trustworthy interpretation and introduce our Predictabiltiy Computability Stability (PCS) framework for such a data science life cycle