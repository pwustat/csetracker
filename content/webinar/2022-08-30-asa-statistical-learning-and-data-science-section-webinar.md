---
title: 'ASA Statistical Learning and Data Science Section Webinar: Exploration and
  Optimization in Deep Reinforcement Learning'
author: ''
date: '2022-08-30T11:00:00'
slug: asa-statistical-learning-and-data-science-section-webinar
categories: []
tags: []
type: webinar
url_freeregister: https://www.eventbrite.com/e/exploration-and-optimization-in-deep-reinforcement-learning-tickets-407170015477
url_register: ~
url_slides: ~
url_video: no
url_agenda: ~
url_website: ~
url_audio: ~
url_code: ~
url_pdf: ~
date_end: '2022-08-30T12:30:00'
all_day: no
publishDate: '2022-09-12T10:47:02-07:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
slides: ''
projects: []
location: ~
address:
  street: ~
  city: ~
  region: ~
  postcode: ~
  country: ~
summary: ~
abstract: ~
speaker: Linglong Kong (University of Alberta)
---

<!--more-->
Reinforcement Learning (RL) is a mathematical framework to develop intelligent agents that can learn the optimal behaviour that maximizes the cumulative reward by interacting with the environment. There are numerous successful applications in many fields. Statistics and optimization are becoming important tools for RL. In this talk, we will look at two of our recent developments. In the first example, we employ distributional RL for efficient exploration. In distributional RL, the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components: a decaying schedule to suppress the intrinsic uncertainty and an exploration bonus calculated from the upper quantiles of the learned distribution. In the second example, we study damped Anderson mixing for deep RL. Anderson mixing has been heuristically applied to RL algorithms for accelerating convergence and improving the sampling efficiency of deep RL. Motivated by that, we provide a rigorous mathematical justification for the benefits of Anderson mixing in RL. Our main results establish a connection between Anderson mixing and quasi-Newton methods, prove that Anderson mixing increases the convergence radius of policy iteration schemes by an extra contraction factor, and propose a stabilization strategy. Besides the two examples, we will discuss some current progress and future directions on statistics and optimization in RL.